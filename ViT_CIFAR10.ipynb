{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8522df-012a-4477-aa56-8c440af1fe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (4.62.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.8/site-packages (1.6.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install transformers datasets\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8480377-33d1-4672-9438-61903ff5ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot\n",
    "from numpy import inf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, ViTFeatureExtractor, ViTModel\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d201fbe0-0c12-4f20-a376-8e707d1b0af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse', 'dog', 'ship', 'frog', 'truck', 'deer', 'airplane', 'automobile', 'cat', 'bird']\n",
      "['horse', 'dog', 'ship', 'frog', 'truck', 'deer', 'airplane', 'automobile', 'cat', 'bird']\n",
      "42500 7501\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader,random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "data_dir = './data/cifar10'\n",
    "\n",
    "print(os.listdir(path = data_dir + '/train'))\n",
    "classes = os.listdir(path = data_dir + '/train')\n",
    "print(classes)\n",
    "train_dir = data_dir + '/train'\n",
    "test_dir = data_dir + '/test'\n",
    "\n",
    "train_dataset = ImageFolder(root=train_dir,transform=transforms.Compose([\n",
    "                                                        transforms.Resize((224,224)),\n",
    "                                                        transforms.ToTensor()]))\n",
    "test_dataset = ImageFolder(root=test_dir,transform=transforms.Compose([\n",
    "                                                        transforms.Resize((224,224)),\n",
    "                                                        transforms.ToTensor()]))\n",
    "\n",
    "Data_available = len(train_dataset)\n",
    "training_data_size = int(0.85*Data_available)\n",
    "validation_data_size = Data_available - training_data_size\n",
    "print(training_data_size,validation_data_size)\n",
    "train_dl,val_dl = random_split(train_dataset,lengths=[training_data_size,validation_data_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d469e2-b35a-411f-a0b6-408904507704",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create dataloaders\n",
    "train_batch_size = 64\n",
    "eval_batch_size = train_batch_size*2\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dl, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dl, batch_size=eval_batch_size, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579d3787-8234-4c3d-80ab-b9e2e0ae896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c37f6144284a619368a74829f9d2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7fd0a1b70a4eb4b35006747efe64dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "for param in vit_model.parameters():\n",
    "      param.requires_grad = False\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=10):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = vit_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size,num_labels)\n",
    "        self.num_labels = num_labels\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "model = ViTForImageClassification()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425fc6a3-b641-4d65-a092-f4a518e6f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc751a0-fee5-45b7-b4f1-04d5f3fc94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "# number of training epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba5c740-b8d2-4116-bf4b-70afbc3fa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    # iterate over batches\n",
    "    for step,(pix,lbl) in enumerate(train_dataloader):\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        # print(batch)\n",
    "        # lbl, pix = batch\n",
    "        lbl, pix = lbl.to(device), pix.to(device)\n",
    "        #print(pix.shape)\n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        preds = model(pix)\n",
    "       \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, lbl)\n",
    "        \n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # clear calculated gradients\n",
    "        optimizer.zero_grad()  \n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        \n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9018ad-071c-495e-9916-4007f0228193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    total_loss = 0\n",
    "    model.eval() # prep model for evaluation\n",
    "    for step,(pix,lbl) in enumerate(val_dataloader):\n",
    "        lbl, pix = lbl.to(device), pix.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        preds = model(pix)\n",
    "        # calculate the loss\n",
    "        loss = cross_entropy(preds, lbl)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b1966d-b7c6-48d7-af15-9a2b93fe62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.519\n",
      "\n",
      " Validation Loss: 0.294\n",
      "\n",
      " Epoch 2 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.271\n",
      "\n",
      " Validation Loss: 0.255\n",
      "\n",
      " Epoch 3 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.241\n",
      "\n",
      " Validation Loss: 0.239\n",
      "\n",
      " Epoch 4 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.225\n",
      "\n",
      " Validation Loss: 0.232\n",
      "\n",
      " Epoch 5 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.215\n",
      "\n",
      " Validation Loss: 0.226\n",
      "\n",
      " Epoch 6 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.205\n",
      "\n",
      " Validation Loss: 0.223\n",
      "\n",
      " Epoch 7 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.198\n",
      "\n",
      " Validation Loss: 0.220\n",
      "\n",
      " Epoch 8 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.194\n",
      "\n",
      " Validation Loss: 0.218\n",
      "\n",
      " Epoch 9 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.189\n",
      "\n",
      " Validation Loss: 0.217\n",
      "\n",
      " Epoch 10 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.185\n",
      "\n",
      " Validation Loss: 0.216\n",
      "\n",
      " Epoch 11 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.181\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 12 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.180\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 13 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.177\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 14 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.173\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 15 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.171\n",
      "\n",
      " Validation Loss: 0.212\n",
      "\n",
      " Epoch 16 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.170\n",
      "\n",
      " Validation Loss: 0.213\n",
      "\n",
      " Epoch 17 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.169\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 18 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.168\n",
      "\n",
      " Validation Loss: 0.213\n",
      "\n",
      " Epoch 19 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "\n",
      " Training Loss: 0.166\n",
      "\n",
      " Validation Loss: 0.214\n",
      "\n",
      " Epoch 20 / 50\n",
      "  Batch    50  of    665.\n",
      "  Batch   100  of    665.\n",
      "  Batch   150  of    665.\n",
      "  Batch   200  of    665.\n",
      "  Batch   250  of    665.\n",
      "  Batch   300  of    665.\n",
      "  Batch   350  of    665.\n",
      "  Batch   400  of    665.\n",
      "  Batch   450  of    665.\n",
      "  Batch   500  of    665.\n",
      "  Batch   550  of    665.\n",
      "  Batch   600  of    665.\n",
      "  Batch   650  of    665.\n",
      "Early stopping with train_loss:  0.1650005100463006 and val_loss for this epoch:  0.21317589156708475 ...\n"
     ]
    }
   ],
   "source": [
    "min_loss = inf\n",
    "es = 0\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "for epoch in range(epochs):     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # Train model\n",
    "    train_loss, _ = train()\n",
    "    val_loss = eval()\n",
    "    \n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(val_loss)\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        es = 0\n",
    "    else:\n",
    "        es += 1\n",
    "        if es > 4:\n",
    "            print(\"Early stopping with train_loss: \", train_loss, \"and val_loss for this epoch: \", val_loss, \"...\")\n",
    "            break\n",
    "    \n",
    "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f'\\n Training Loss: {train_loss:.3f}')\n",
    "    print(f'\\n Validation Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf10ee76-574c-4350-b048-b9c23ad2a2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f39ddea8e50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_loss,label=\"Training Loss\")\n",
    "plt.plot(validation_loss,label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eda8e8d-ea36-46e1-a569-db6bfd85ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6f89a3-c34d-4941-b6c6-6af70522071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:14<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def eval():\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for step, (pix,lbl) in tqdm(enumerate(test_dataloader), total = len(test_dataloader)):\n",
    "            lbl, pix = lbl.to(device), pix.to(device)\n",
    "            outputs = model(pix)\n",
    "            outputs = torch.argmax(outputs, axis=1)\n",
    "            y_pred.extend(outputs.cpu().detach().numpy())\n",
    "            y_true.extend(lbl.cpu().detach().numpy())\n",
    "            \n",
    "    return y_pred, y_true\n",
    "y_pred, y_true = eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "101982a6-db36-4a9c-8ec5-8e56392a1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model 0.9301\n"
     ]
    }
   ],
   "source": [
    "correct = np.array(y_pred) == np.array(y_true)\n",
    "accuracy = correct.sum() / len(correct)\n",
    "print(\"Accuracy of the model\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
